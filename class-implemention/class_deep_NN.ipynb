{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dff82041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "979d1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this activation functions classes \n",
    "class sigmoid : \n",
    "    def forword(self , inputs) : # sigmoid\n",
    "        return 1 / (1+np.exp(-inputs)) \n",
    "    def backword(self , inputs ): # sigmoid derivative\n",
    "         return inputs * (1-inputs)\n",
    "class relu : \n",
    "    def forword(self ,inputs) : # relu\n",
    "        mask = (inputs > 0 )\n",
    "        return inputs * mask\n",
    "    def backword(self , inputs ): # relu derivative\n",
    "        mask = (inputs > 0)\n",
    "        return inputs * mask\n",
    "class tanh : \n",
    "    def forword(self, inputs) : # tanh \n",
    "        return np.tanh(inputs)\n",
    "    def backword(self, inputs ) : # tanh derivative \n",
    "        return 1 - (inputs**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f45558a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this optimizers classes  \n",
    "class gradient_decent : # this normal gradient decent \n",
    "    def __init__(self, lr = .001 ): \n",
    "        self.lr = lr # learning rate\n",
    "    def back_prop(self , dAL, objects , l , lambd , m  , activation ) : \n",
    "        '''\n",
    "            objects : it,s contain all weights and baises and activatoins from the original model \n",
    "            dAl : the gradients of the loss with respect to weights \n",
    "            l : number of layers \n",
    "            lambd : regularization prameter \n",
    "            m : training examples \n",
    "            activation : the activation function \n",
    "        '''\n",
    "        # compute the gradients for the weights and biases for the output layer\n",
    "        vars(self)[f'dZ_{l}'] = dAL * sigmoid.backword(objects[f'A_{l}']) \n",
    "        vars(self)[f'dW_{l}'] =1. /m * np.dot(  vars(self)[f'dZ_{l}'] , objects[f'A_{l-1}'].T) + (lambd / m ) * objects[f'W_{l}'] \n",
    "        vars(self)[f'db_{l}'] = np.sum(vars(self)[f'dZ_{l}'] , axis = 1, keepdims =True) / m\n",
    "        vars(self)[f'dA_{l-1}'] = np.dot(objects[f'W_{l}'].T ,vars(self)[f'dZ_{l}'] ) \n",
    "        for i in reversed(range(1 , l)) : # iteraite backword throw the hidden layers \n",
    "        # compute the gradients for weights and biases for the hidden layers  \n",
    "            vars(self)[f'dZ_{i}'] = vars(self)[f'dA_{i}'] * activation.backword(objects[f'A_{i}'])\n",
    "            vars(self)[f'dW_{i}'] =1. / m *  np.dot( vars(self)[f'dZ_{i}'] ,objects[f'A_{i-1}'].T) + (lambd / m ) * objects[f'W_{i}']\n",
    "            vars(self)[f'db_{i}'] = np.sum(vars(self)[f'dZ_{i}'] , keepdims= True , axis =1) /m\n",
    "            vars(self)[f'dA_{i-1}'] = np.dot(objects[f'W_{i}'].T ,vars(self)[f'dZ_{i}'] )\n",
    "      \n",
    "      # update the weights and biases\n",
    "        for i in range(1, l+1) : \n",
    "            objects[f'W_{i}'] -= self.lr * vars(self)[f'dW_{i}']\n",
    "            objects[f'b_{i}'] -= self.lr * vars(self)[f'db_{i}']\n",
    "class Adam  :  \n",
    "    \n",
    "    def __init__(self, beta1 = .9  ,lr = .001, beta2 = .999 ,  epsilon= 1e-8) : \n",
    "        \n",
    "        self.lr = lr  # learning rate \n",
    "        self.beta1 = beta1 \n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "    def build(self , objects , l ) : \n",
    "        for i in range( 1 , l+1 ): \n",
    "            # init the weights for the optimizer \n",
    "            vars(self)[f'vdW_{i}'] = np.zeros((objects[f'W_{i}'].shape))\n",
    "            vars(self)[f'vdb_{i}'] = np.zeros((objects[f'b_{i}'].shape))\n",
    "            vars(self)[f'sdW_{i}'] = np.zeros((objects[f'W_{i}'].shape))\n",
    "            vars(self)[f'sdb_{i}'] = np.zeros((objects[f'b_{i}'].shape))\n",
    "    def back_prop(self , dAL, objects , l , lambd , m  , activation ) :\n",
    "         '''\n",
    "            objects : it,s contain all weights and baises and activatoins from the original model \n",
    "            dAl : the gradients of the loss with respect to weights \n",
    "            l : number of layers \n",
    "            lambd : regularization prameter \n",
    "            m : training examples \n",
    "            activation : the activation function \n",
    "        '''\n",
    "        # call method build to init the weights \n",
    "        self.build(objects , l )\n",
    "        # compute the gradients of the output layer \n",
    "        vars(self)[f'dZ_{l}'] = dAL * sigmoid.backword(objects[f'A_{l}']) \n",
    "        vars(self)[f'dW_{l}'] =1. /m * np.dot(  vars(self)[f'dZ_{l}'] , objects[f'A_{l-1}'].T) + (lambd / m ) * objects[f'W_{l}'] \n",
    "        vars(self)[f'db_{l}'] = np.sum(vars(self)[f'dZ_{l}'] , axis = 1, keepdims =True) / m\n",
    "        vars(self)[f'vdW_{l}'] = self.beta1 * vars(self)[f'vdW_{l}'] + (1- self.beta1) * vars(self)[f'dW_{l}']\n",
    "        vars(self)[f'vdb_{l}'] = self.beta1 * vars(self)[f'vdb_{l}'] + (1- self.beta1) * vars(self)[f'db_{l}']\n",
    "        vars(self)[f'sdW_{l}'] = self.beta2 * vars(self)[f'sdW_{l}'] + (1-self.beta2) * (vars(self)[f'dW_{l}'] ** 2)\n",
    "        vars(self)[f'sdb_{l}'] = self.beta2 * vars(self)[f'sdb_{l}'] + (1-self.beta2) * (vars(self)[f'db_{l}'] ** 2)\n",
    "        vars(self)[f'dA_{l-1}'] = np.dot(objects[f'W_{l}'].T ,vars(self)[f'dZ_{l}'] ) \n",
    "        for i in reversed(range(1 , l)) : # iteraite backword throw the hidden layers \n",
    "        # compute the gradients for weights and biases for the hidden layers  \n",
    "            vars(self)[f'dZ_{i}'] = vars(self)[f'dA_{i}'] * activation.backword(objects[f'A_{i}'])\n",
    "            vars(self)[f'dW_{i}'] =1. / m *  np.dot( vars(self)[f'dZ_{i}'] ,objects[f'A_{i-1}'].T) + (lambd / m ) * objects[f'W_{i}']\n",
    "            vars(self)[f'db_{i}'] = np.sum(vars(self)[f'dZ_{i}'] , keepdims= True , axis =1) /m\n",
    "            vars(self)[f'vdW_{i}'] = self.beta1 * vars(self)[f'vdW_{i}'] + (1- self.beta1) * vars(self)[f'dW_{i}']\n",
    "            vars(self)[f'vdb_{i}'] = self.beta1 * vars(self)[f'vdb_{i}'] + (1- self.beta1) * vars(self)[f'db_{i}']\n",
    "            vars(self)[f'sdW_{i}'] = self.beta2 * vars(self)[f'sdW_{i}'] + (1-self.beta2) * (vars(self)[f'dW_{i}'] ** 2)\n",
    "            vars(self)[f'sdb_{i}'] = self.beta2 * vars(self)[f'sdb_{i}'] + (1-self.beta2) * (vars(self)[f'db_{i}'] ** 2)\n",
    "            vars(self)[f'dA_{i-1}'] = np.dot(objects[f'W_{i}'].T ,vars(self)[f'dZ_{i}'] )\n",
    "      \n",
    "      # update the weights and biases\n",
    "        for i in range(1, l+1) : \n",
    "            objects[f'W_{i}'] -= self.lr * (vars(self)[f'vdW_{i}']/ (np.sqrt(vars(self)[f'sdW_{i}'] + self.epsilon)))\n",
    "            objects[f'b_{i}'] -= self.lr * (vars(self)[f'vdb_{i}']/ (np.sqrt(vars(self)[f'sdb_{i}'] + self.epsilon)))\n",
    "class rmsprop  :  \n",
    "    def __init__(self, beta = .9  ,lr = .001  ,epsilon= 1e-8) : \n",
    "        self.lr = lr \n",
    "        self.beta1 = beta1\n",
    "        self.epsilon = epsilon\n",
    "    def build(self , objects , l ) : \n",
    "        for i in range( 1 , l+1 ): \n",
    "            vars(self)[f'sdW_{i}'] = np.zeros((objects[f'W_{i}'].shape))\n",
    "            vars(self)[f'sdb_{i}'] = np.zeros((objects[f'b_{i}'].shape))\n",
    "    def back_prop(self , dAL, objects , l , lambd , m  , activation ) : \n",
    "         '''\n",
    "            objects : it,s contain all weights and baises and activatoins from the original model \n",
    "            dAl : the gradients of the loss with respect to weights \n",
    "            l : number of layers \n",
    "            lambd : regularization prameter \n",
    "            m : training examples \n",
    "            activation : the activation function \n",
    "        '''\n",
    "        # call method build \n",
    "        self.build(objects , l )\n",
    "        \n",
    "        vars(self)[f'dZ_{l}'] = dAL * sigmoid.backword(objects[f'A_{l}']) \n",
    "        vars(self)[f'dW_{l}'] =1. /m * np.dot(  vars(self)[f'dZ_{l}'] , objects[f'A_{l-1}'].T) + (lambd / m ) * objects[f'W_{l}'] \n",
    "        vars(self)[f'db_{l}'] = np.sum(vars(self)[f'dZ_{l}'] , axis = 1, keepdims =True) / m\n",
    "        vars(self)[f'sdW_{l}'] = self.beta2 * vars(self)[f'sdW_{l}'] + (1-self.beta2) * (vars(self)[f'dW_{l}'] ** 2)\n",
    "        vars(self)[f'sdb_{l}'] = self.beta2 * vars(self)[f'sdb_{l}'] + (1-self.beta2) * (vars(self)[f'db_{l}'] ** 2)\n",
    "        vars(self)[f'dA_{l-1}'] = np.dot(objects[f'W_{l}'].T ,vars(self)[f'dZ_{l}'] ) \n",
    "        for i in reversed(range(1 , l)) : # iteraite backword throw the hidden layers \n",
    "        # compute the gradients for weights and biases for the hidden layers  \n",
    "            vars(self)[f'dZ_{i}'] = vars(self)[f'dA_{i}'] * activation.backword(objects[f'A_{i}'])\n",
    "            vars(self)[f'dW_{i}'] =1. / m *  np.dot( vars(self)[f'dZ_{i}'] ,objects[f'A_{i-1}'].T) + (lambd / m ) * objects[f'W_{i}']\n",
    "            vars(self)[f'db_{i}'] = np.sum(vars(self)[f'dZ_{i}'] , keepdims= True , axis =1) /m\n",
    "            vars(self)[f'sdW_{i}'] = self.beta2 * vars(self)[f'sdW_{i}'] + (1-self.beta2) * (vars(self)[f'dW_{i}'] ** 2)\n",
    "            vars(self)[f'sdb_{i}'] = self.beta2 * vars(self)[f'sdb_{i}'] + (1-self.beta2) * (vars(self)[f'db_{i}'] ** 2)\n",
    "            vars(self)[f'dA_{i-1}'] = np.dot(objects[f'W_{i}'].T ,vars(self)[f'dZ_{i}'] )\n",
    "      \n",
    "      # update the weights and biases\n",
    "        for i in range(1, l+1) : \n",
    "            objects[f'W_{i}'] -= self.lr * (objects[f'dW_{i}']/ (np.sqrt(vars(self)[f'sdW_{i}'] + self.epsilon)))\n",
    "            objects[f'b_{i}'] -= self.lr * (objects[f'db_{i}']/ (np.sqrt(vars(self)[f'sdb_{i}'] + self.epsilon)))\n",
    "   \n",
    "            \n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "278a188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_nn_model : \n",
    "    def __init__(self , l ,layer_dims , activation ):\n",
    "        '''\n",
    "        l : number of layers \n",
    "        layer_dims : list containing the hidden units for each hidden layer for example [num_features ,128 ,64,32 ]\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.l = l \n",
    "        self.layer_dims = layer_dims \n",
    "    def build(self): \n",
    "        # init the weights and baises \n",
    "        for i in range(1, self.l + 1) : \n",
    "            vars(self)[f'W_{i}'] = np.random.randn(self.layer_dims[i] ,self.layer_dims[i-1] ) * np.sqrt(1/self.layer_dims[i-1])\n",
    "            vars(self)[f'b_{i}'] = np.zeros((self.layer_dims[i] , 1))\n",
    "    def fit(self, X , Y   , lambd = .9 , num_iter = 2000) : \n",
    "        ''' \n",
    "        X : input data with shape(num_features , num_examples) \n",
    "        Y : input labels with shape(1 , num_examples)\n",
    "        lambd : the regularization parameter \n",
    "        '''\n",
    "        self.build()\n",
    "        activation = self.activation\n",
    "        losses = []\n",
    "        m = X.shape[-1]\n",
    "        # assign the X to activation 0 \n",
    "        vars(self)[f'A_0'] = X\n",
    "        for _ in range(num_iter): # compute the forword pass \n",
    "            for i in range(1,self.l) : \n",
    "                Z = np.dot(vars(self)[f'W_{i}'] , vars(self)[f'A_{i-1}']) + vars(self)[f'b_{i}'] \n",
    "                vars(self)[f'A_{i}'] = activation.forword(Z)\n",
    "            lastZ = np.dot(vars(self)[f'W_{self.l}'] , vars(self)[f'A_{self.l-1}']) + vars(self)[f'b_{self.l}'] \n",
    "            vars(self)[f'A_{self.l}'] = sigmoid.forword(lastZ)\n",
    "            # compute the loss \n",
    "            loss = - (np.dot(Y , np.log(vars(self)[f'A_{self.l}'].T) + np.dot((1-Y) , np.log((1-vars(self)[f'A_{self.l}']).T))) /m)\n",
    "            l2_reg = 0\n",
    "            # the l2 reqularization \n",
    "            for i in range(1 , self.l+1) : \n",
    "                l2_reg += np.sum(np.square(vars(self)[f'W_{i}'])) \n",
    "            l2_reg = lambd * l2_reg / (2 * m)\n",
    "            loss = loss + l2_reg\n",
    "            losses.append(loss)\n",
    "            # the drevitve of the loss with respect the last activation \n",
    "            dAL = - (np.divide(Y, vars(self)[f'A_{self.l}' ]) - np.divide(1 -Y , 1 - vars(self)[f'A_{self.l}']))\n",
    "            opt.back_prop(  dAL = dAL, objects = vars(self) ,  l = self.l , lambd = lambd  ,m =  m , activation = activation  )\n",
    "            \n",
    "    def predict(self  , X ) : \n",
    "        activation = self.activation\n",
    "        vars(self)[f'AP_{0}'] = X \n",
    "        for i in range(1,self.l) : \n",
    "            ZP = np.dot(vars(self)[f'W_{i}'] , vars(self)[f'AP_{i-1}']) + vars(self)[f'b_{i}'] \n",
    "            vars(self)[f'AP_{i}'] = activation.forword(ZP)\n",
    "        last_PZ = np.dot(vars(self)[f'W_{self.l}'] , vars(self)[f'AP_{self.l-1}']) + vars(self)[f'b_{self.l}'] \n",
    "        vars(self)[f'AP_{self.l}'] = sigmoid.forword(last_PZ)\n",
    "        return vars(self)[f'AP_{self.l}']\n",
    "    def evaluate(self , preds , true_y ) : \n",
    "        num_correct = 0 \n",
    "        preds = (preds > .5) \n",
    "        for i in range(true_y.shape[0]) : \n",
    "             if preds[0][i] == true_y[i] : \n",
    "                num_correct += 1 \n",
    "        return num_correct / true_y.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "73d1400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "import tensorflow \n",
    "(train_data , train_labels ) , (test_data , test_labels) = tensorflow.keras.datasets.mnist.load_data()\n",
    "train_data , test_data = train_data / 255 , test_data /255\n",
    "train_data = train_data.reshape((60000 , 28*28))\n",
    "# take subset of the train data to train on because the dataset is so big\n",
    "train_subset = train_data[:5000 , :]\n",
    "train_subset_label = train_labels[:5000]\n",
    "\n",
    "# because the mnist is multiclass problem and we want it to be binary we will change it \n",
    "# instead of predicting the number we will predict if this number is odd or even\n",
    "\n",
    "# convert the label to binary\n",
    "for i in range(5000) : \n",
    "  if train_subset_label[i] // 2 == 0 :\n",
    "    train_subset_label[i] = 0\n",
    "  else :\n",
    "    train_subset_label[i] = 1\n",
    "# take subset of the train data to test on because the dataset is so big\n",
    "test_subset = train_data[5001 :6001 , :]  # i took the subset from the training data but you can take it from the test data\n",
    "test_subset_label = train_labels[5001 : 6001]\n",
    "\n",
    "# because the mnist is multiclass problem and we want it to be binary we will change it \n",
    "# instead of predicting the number we will predict if this number is odd or even\n",
    "\n",
    "# convert the label to binary\n",
    "for i in range(1000) : \n",
    "    if test_subset_label[i] // 2 == 0 :\n",
    "        test_subset_label[i] = 0\n",
    "    else :\n",
    "        test_subset_label[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4f1ebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = sigmoid()\n",
    "tanh = tanh()\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9c5815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deep_nn_model(3 , [784 , 64 ,32 ,1] , activation = tanh )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "99e5be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_subset.T, train_subset_label.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a989739",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_subset.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ab6cab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate( preds  ,test_subset_label.T )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
