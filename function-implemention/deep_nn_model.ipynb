{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_nn_model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Bm74n5vGVWE"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# those some helper activation functions and thier derivatives \n",
        "def sigmoid(input):  # sigmoid \n",
        "  return 1 / (1+np.exp(-input)) \n",
        "def sigmoid_d(output) : # sigmoid derivative\n",
        "  return output * (1-output)\n",
        "def tanh(input) : # tanh  \n",
        "  return np.tanh(input)\n",
        "def tanh_d(output): # tanh derivative \n",
        "  return 1 - (output**2) \n",
        "def relu(input) : # relu\n",
        "  mask = (input > 0 )\n",
        "  return input * mask\n",
        "def relu_d(output): # relu derivative\n",
        "  mask = (output > 0)\n",
        "  return output * mask"
      ],
      "metadata": {
        "id": "4hbfjuHVUnO3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_nn(X, Y, l ,layer_dims ,lr =.001 , num_iter =4000 , act_function = tanh , act_deriv = tanh_d , lambd = .1 , optimizer = 'adam' ,beta1 = .9, beta2= .999 , epsilon= 1e-8) :\n",
        "  '''\n",
        "    inputs :\n",
        "    X : input data with shape(num_features , num_examples) \n",
        "    Y : input labels with shape(1 , num_examples)\n",
        "    l : number of layers \n",
        "    layer_dims : list containing the hidden units for each hidden layer for example [num_features ,128 ,64,32 ]\n",
        "    lr :learning_rate \n",
        "    num_iter : number of  iteration for training\n",
        "    act_function : the activation function for forword propagation \n",
        "    act_deriv : the derivative of the activaion function for backowrd propagation \n",
        "    lambd : regularization parameter \n",
        "    optimizer : optimizer name \n",
        "    beta1 : optimization parameter\n",
        "    beta2 : optimization parameter\n",
        "    epsilon : small number to add to avoid dividing by zero   \n",
        "  '''\n",
        "  parameters = {}  # parameter dict \n",
        "  activations = {} # activation dict to keep the activation because we need them throw the back propagation\n",
        "  losses =[] # list to store the losses \n",
        "  grads = {} # gradients dict \n",
        "  m = X.shape[-1] # number of training examples \n",
        "  activations['A' + str(0)] = X  # we set the first activation to the input data \n",
        "  \n",
        "  for i in range(1, l+1 ):  # initialize the weights and biases\n",
        "    parameters['W' + str(i)] = np.random.randn(layer_dims[i] ,layer_dims[i-1] ) * np.sqrt(1/layer_dims[i-1])\n",
        "    parameters['b' + str(i)] = np.zeros((layer_dims[i] , 1))\n",
        "  \n",
        "  \n",
        "  if optimizer =='rms': # this initializtion related to rms_prob optimizer you can back to it leter \n",
        "    s ={}\n",
        "    for i in range(1 , l+1) : \n",
        "      s[\"dW\" + str(i)] = np.zeros((parameters['W' + str(i)].shape))\n",
        "      s[\"db\" + str(i)] = np.zeros((parameters['b' + str(i)].shape))\n",
        "  if optimizer == 'adam' : # this initializtion related to adam optimizer you can back to it leter \n",
        "    v = {}\n",
        "    s = {}\n",
        "    for i in range(1 , l+1) :\n",
        "        v[\"dW\" + str(i)] = np.zeros((parameters['W' + str(i)].shape))\n",
        "        v[\"db\" + str(i)] = np.zeros((parameters['b' + str(i)].shape))\n",
        "        s[\"dW\" + str(i)] = np.zeros((parameters['W' + str(i)].shape))\n",
        "        s[\"db\" + str(i)] = np.zeros((parameters['b' + str(i)].shape))\n",
        "\n",
        "  \n",
        "  for it in range(num_iter) : # iterating  \n",
        "    for i in range(1, l) : # compute the forword propagation for each layer except the output layer\n",
        "      Z = np.dot(parameters['W' + str(i)] , activations['A' + str(i-1)]) + parameters['b'+ str(i)] \n",
        "      activations['A' + str(i)] = act_function(Z) \n",
        "    # compute the output layer \n",
        "    lastZ = np.dot(parameters['W' + str(l)] , activations['A' + str(l-1)]) + parameters['b' + str(l)] \n",
        "    activations['A' + str(l)] = sigmoid(lastZ)\n",
        "\n",
        "    # compute the losses and add the regularization \n",
        "    loss = - (np.dot(Y , np.log(activations['A' + str(l)]).T) + np.dot((1-Y) , np.log((1-activations['A' + str(l)]).T))) /m\n",
        "    l2_reg = 0\n",
        "    for i in range(1 , l+1) : \n",
        "      l2_reg += np.sum(np.square(parameters['W' + str(i)])) \n",
        "    l2_reg = lambd * l2_reg / (2 * m)\n",
        "    loss = loss + l2_reg\n",
        "    losses.append(loss)\n",
        "\n",
        "    # compute the derivative of the outputs with respect to the loss \n",
        "    dAL = - (np.divide(Y, activations['A' + str(l)]) - np.divide(1 -Y , 1 - activations['A'+ str(l)])) \n",
        "    \n",
        "    # check the kind of optimizer \n",
        "    if optimizer =='gd' :\n",
        "      # compute the gradients for the weights and biases for the output layer\n",
        "      grads['dZ' + str(l)] = dAL * sigmoid_d(activations['A' + str(l)])\n",
        "      grads['dW' + str(l)] =1. /m * np.dot(  grads['dZ' + str(l)] , activations['A' + str(l-1)].T) + (lambd / m ) * parameters['W' + str(l)] \n",
        "      grads['db' + str(l)] = np.sum(grads['dZ' + str(l)] , axis = 1, keepdims =True) / m\n",
        "      grads['dA' + str(l-1)] = np.dot(parameters['W' + str(l)].T ,grads['dZ' + str(l)] )  \n",
        "\n",
        "      for i in reversed(range(1 , l)) : # iteraite backword throw the hidden layers \n",
        "        # compute the gradients for weights and biases for the hidden layers  \n",
        "        grads['dZ'+ str(i)] = grads['dA' + str(i)] * act_deriv(activations['A' + str(i)])\n",
        "        grads['dW' + str(i)] =1. / m *  np.dot( grads['dZ'+ str(i)] ,activations['A' + str (i-1)].T) + (lambd / m ) * parameters['W' + str(i)]\n",
        "        grads['db' + str(i)] = np.sum(grads['dZ'+ str(i)] , keepdims= True , axis =1) /m\n",
        "        grads['dA' + str(i-1)] = np.dot(parameters['W' + str(i)].T ,grads['dZ' + str(i)] )\n",
        "      \n",
        "      # update the weights and biases\n",
        "      for i in range(1, l+1) : \n",
        "        parameters['W' + str(i)] -= lr * grads['dW' + str(i)]\n",
        "        parameters['b' + str(i)] -= lr * grads['db' + str(i)]\n",
        "    \n",
        "    \n",
        "    elif optimizer == 'rms' : # rms_prob\n",
        "      # compute the gradients for the weights and biases for the output layer\n",
        "      grads['dZ' + str(l)] = dAL * sigmoid_d(activations['A' + str(l)])\n",
        "      grads['dW' + str(l)] =1. /m * np.dot(  grads['dZ' + str(l)] , activations['A' + str(l-1)].T) + (lambd / m ) * parameters['W' + str(l)] \n",
        "      grads['db' + str(l)] = np.sum(grads['dZ' + str(l)] , axis = 1, keepdims =True) / m\n",
        "      s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1-beta2) * (grads['dW' + str(l)] ** 2)\n",
        "      s['db' + str(l)] = beta2 * s['db' + str(l)] + (1-beta2) * (grads['db' + str(l)] ** 2)\n",
        "      grads['dA' + str(l-1)] = np.dot(parameters['W' + str(l)].T ,grads['dZ' + str(l)] ) \n",
        "      \n",
        "      for i in reversed(range(1 , l)) : \n",
        "        # compute the gradients for weights and biases for the hidden layers\n",
        "        grads['dZ'+ str(i)] = grads['dA' + str(i)] * act_deriv(activations['A' + str(i)])\n",
        "        grads['dW' + str(i)] =1. / m *  np.dot( grads['dZ'+ str(i)] ,activations['A' + str (i-1)].T) + (lambd / m ) * parameters['W' + str(i)]\n",
        "        grads['db' + str(i)] = np.sum(grads['dZ'+ str(i)] , keepdims= True , axis =1) /m\n",
        "        s['dW' + str(i)] = beta2 * s['dW' + str(i)] + (1-beta2) * (grads['dW' + str(i)] ** 2)\n",
        "        s['db' + str(i)] = beta2 * s['db' + str(i)] + (1-beta2) * (grads['db' + str(i)] ** 2)\n",
        "        grads['dA' + str(i-1)] = np.dot(parameters['W' + str(i)].T ,grads['dZ' + str(i)] )\n",
        "      \n",
        "      # update the parameters \n",
        "      for i in range(1, l+1) : \n",
        "        parameters['W' + str(i)] -= lr * (grads['dW' + str(i)] / (np.sqrt(s['dW' + str(i)]) + epsilon))\n",
        "        parameters['b' + str(i)] -= lr * (grads['db' + str(i)] / (np.sqrt(s['db' + str(i)]) + epsilon))\n",
        "    \n",
        "    elif optimizer =='adam' : # adam optimizer\n",
        "        # compute the gradients for the weights and biases for the output layer\n",
        "        grads['dZ' + str(l)] = dAL * sigmoid_d(activations['A' + str(l)])\n",
        "        grads['dW' + str(l)] =1. /m * np.dot(  grads['dZ' + str(l)] , activations['A' + str(l-1)].T) + (lambd / m ) * parameters['W' + str(l)] \n",
        "        grads['db' + str(l)] = np.sum(grads['dZ' + str(l)] , axis = 1, keepdims =True) / m\n",
        "        v['dW' + str(l)] = beta1 * v['dW' + str(l)] + (1- beta1) * grads['dW' + str(l)]\n",
        "        v['db' + str(l)] = beta1 * v['db' + str(l)] + (1- beta1) * grads['db' + str(l)]\n",
        "        s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1-beta2) * (grads['dW' + str(l)] ** 2)\n",
        "        s['db' + str(l)] = beta2 * s['db' + str(l)] + (1-beta2) * (grads['db' + str(l)] ** 2)\n",
        "        grads['dA' + str(l-1)] = np.dot(parameters['W' + str(l)].T ,grads['dZ' + str(l)] ) \n",
        "        \n",
        "        for i in reversed(range(1 , l)) : \n",
        "           # compute the gradients for weights and biases for the hidden layers\n",
        "          grads['dZ'+ str(i)] = grads['dA' + str(i)] * act_deriv(activations['A' + str(i)])\n",
        "          grads['dW' + str(i)] =1. / m *  np.dot( grads['dZ'+ str(i)] ,activations['A' + str (i-1)].T) + (lambd / m ) * parameters['W' + str(i)]\n",
        "          grads['db' + str(i)] = np.sum(grads['dZ'+ str(i)] , keepdims= True , axis =1) /m\n",
        "          v['dW' + str(i)] = beta1 * v['dW' + str(i)] + (1- beta1) * grads['dW' + str(i)]\n",
        "          v['db' + str(i)] = beta1 * v['db' + str(i)] + (1- beta1) * grads['db' + str(i)]\n",
        "          s['dW' + str(i)] = beta2 * s['dW' + str(i)] + (1-beta2) * (grads['dW' + str(i)] ** 2)\n",
        "          s['db' + str(i)] = beta2 * s['db' + str(i)] + (1-beta2) * (grads['db' + str(i)] ** 2)\n",
        "          grads['dA' + str(i-1)] = np.dot(parameters['W' + str(i)].T ,grads['dZ' + str(i)] )\n",
        "        \n",
        "        # update the parameters\n",
        "        for i in range(1, l+1) : \n",
        "          parameters['W' + str(i)] -= lr * (v['dW' + str(i)]/ (np.sqrt(s['dW' + str(i)]) + epsilon))\n",
        "          parameters['b' + str(i)] -= lr * (v['db' + str(i)] / (np.sqrt(s['db' + str(i)]) + epsilon))\n",
        "  \n",
        "  # return the parameters and losses\n",
        "  return parameters, losses"
      ],
      "metadata": {
        "id": "Om0r3y96wkcs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function to predict and evaulate the model \n",
        "def predict(X , y , parameters) :\n",
        "  '''\n",
        "  X : input data with the same shape as before\n",
        "  y : input labels \n",
        "  parameters : dict containing the updated weights and biases which we will use to predict  \n",
        "  '''\n",
        "\n",
        "  activations ={} # activation dict as before \n",
        "  activations['A0'] = X \n",
        "  num_correct = 0 # we will use that to compute the accuracy \n",
        "  l =int(len(parameters.keys()) /2 )  # we calculate the number of hidden layer \n",
        "  \n",
        "  # predict the output with our updated weights and biases \n",
        "  for i in range(1 , l ) : \n",
        "     Z = np.dot(parameters['W' + str(i)] , activations['A' + str(i-1)]) + parameters['b'+ str(i)] \n",
        "     activations['A' + str(i)] = tanh(Z)\n",
        "  lastZ = np.dot(parameters['W' + str(l)] , activations['A' + str(l-1)]) + parameters['b' + str(l)] \n",
        "  activations['A' + str(l)] = sigmoid(lastZ)\n",
        "\n",
        "  # we convert the output activations to ones or zeros to compare it to the labels\n",
        "  activations['A' + str(l)] = (activations['A' + str(l)] > .5) \n",
        "  \n",
        "  # comparing the outputs to the label\n",
        "  for i in range(y.shape[0]) : \n",
        "    if activations['A' + str(l)][0][i] == y[i] : \n",
        "      num_correct += 1 \n",
        "  # compute the accuracy\n",
        "  acc = num_correct / y.shape[0]\n",
        "  return acc "
      ],
      "metadata": {
        "id": "QnPOB8oOmotM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data \n",
        "import tensorflow \n",
        "(train_data , train_labels ) , (test_data , test_labels) = tensorflow.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "J67sUv73nEZD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "train_data , test_data = train_data / 255 , test_data /255"
      ],
      "metadata": {
        "id": "T6ogu_NTu-vR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape the data \n",
        "train_data = train_data.reshape((60000 , 28*28))"
      ],
      "metadata": {
        "id": "eaMU0cnoo7kI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take subset of the train data to train on because the dataset is so big\n",
        "train_subset = train_data[:5000 , :]\n",
        "train_subset_label = train_labels[:5000]\n",
        "\n",
        "# because the mnist is multiclass problem and we want it to be binary we will change it \n",
        "# instead of predicting the number we will predict if this number is odd or even\n",
        "\n",
        "# convert the label to binary\n",
        "for i in range(5000) : \n",
        "  if train_subset_label[i] // 2 == 0 :\n",
        "    train_subset_label[i] = 0\n",
        "  else :\n",
        "    train_subset_label[i] = 1"
      ],
      "metadata": {
        "id": "Mu9YUgKDpAnC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take subset of the train data to test on because the dataset is so big\n",
        "test_subset = train_data[5001 :6001 , :]  # i took the subset from the training data but you can take it from the test data\n",
        "test_subset_label = train_labels[5001 : 6001]\n",
        "\n",
        "# because the mnist is multiclass problem and we want it to be binary we will change it \n",
        "# instead of predicting the number we will predict if this number is odd or even\n",
        "\n",
        "# convert the label to binary\n",
        "for i in range(1000) : \n",
        "  if test_subset_label[i] // 2 == 0 :\n",
        "    test_subset_label[i] = 0\n",
        "  else :\n",
        "    test_subset_label[i] = 1"
      ],
      "metadata": {
        "id": "MI3Q9TVRu6HR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters , losses = deep_nn(train_subset.T, train_subset_label.T , l = 3 ,layer_dims= [784 ,  64 , 32,1 ] ,optimizer='adam')\n",
        "# we pass the subsets transposed to fit the require shape \n",
        "# we choose 3 layers \n",
        "# the layer dims [784 -> the number of features or the pixels of the images 28*28 , 64 -> hidden_units , 32-> hidden_units , 1 -> the output]\n",
        "# we choose adam as optimizer \n"
      ],
      "metadata": {
        "id": "utKF2VGljZE8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the accuracy on the training subset \n",
        "# you will get 100% accuracy \n",
        "acc = predict(train_subset.T , train_subset_label.T , parameters)\n",
        "# parameters : dict include the weights and biases returned from the model \n",
        "print(acc)"
      ],
      "metadata": {
        "id": "EK8qBHBEmvEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdb64fc-9586-42d5-a257-404a23955a66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the accuracy on the test subset \n",
        "# you will get about 99% \n",
        "acc = predict(test_subset.T , test_subset_label.T , parameters) \n",
        "print(acc) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55zzfj6zxToK",
        "outputId": "8d2883b8-ff9d-437e-8bbe-e17dc356defd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.982\n"
          ]
        }
      ]
    }
  ]
}